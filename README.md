# Repository for EMOTIC (EMOTIon recognition in Context)
(Official Website: [EMOTIC](http://sunai.uoc.edu/emotic/index.html))

<img src="EMOTIC_mean_images/emotic_logo.png" alt="drawing" width="100"/>

## [PAMI-2019 (latest Version)]
+ Dataset download link: [PAMI_19 Dataset](https://drive.google.com/open?id=0B7sjGeF4f3FYQUVlZ3ZOai1ieEU)

+ Baseline CNN model : [PAMI_19 MODEL](https://1drv.ms/u/s!AkYHbdGNmIVCgbYSIcSFYJgcApIRKw?e=slJTZp), where you will also find a pseudo code for testing your images. 

+ Annotations download link: [PAMI_19 Annotations](https://1drv.ms/u/s!AkYHbdGNmIVCgbYJxp1EtUplH6BhSw?e=VUP26u)

## [CVPR-2017 Version]
+ Dataset download link: [CVPR_17 Dataset](https://1drv.ms/u/s!AkYHbdGNmIVCgbYUedjPzUcYlpFzBQ?e=qZSaz4)

+ Baseline CNN model: [CVPR_17 MODEL](https://1drv.ms/u/s!AkYHbdGNmIVCgbYX2EL2jNIbpwcHug?e=T3jNGn), where you will also find a pseudo code for testing your images. 

+ Annotations download link: [CVPR_17 Annotations](https://1drv.ms/u/s!AkYHbdGNmIVCgbYYXYzR4WOtfUI9LA?e=v8OVlS)

## Other related files 
+ Demo for computing the Average Precision (AP) [demoComputeAP](https://1drv.ms/u/s!AkYHbdGNmIVCgbYZB_dY3wuWJou_5A?e=jcsZUj)

+ For Model architecture (same for cvpr and pami publications), you can refer [**emotic_cnn_model_structure.txt**](https://1drv.ms/t/s!AkYHbdGNmIVCgbYV2ymTghehKLdxBg?e=PMyGgc)

## Cite us using our following publications
+ PAMI'19 paper: [Context Based Emotion Recognition using EMOTIC Dataset](https://ieeexplore.ieee.org/document/8713881)

+ CVPR'17 paper: [Emotion Recognition in Context](http://openaccess.thecvf.com/content_cvpr_2017/html/Kosti_Emotion_Recognition_in_CVPR_2017_paper.html)

+ CVPRW'17 paper: [EMOTIC: Emotions in Context Dataset](http://openaccess.thecvf.com/content_cvpr_2017_workshops/w41/html/Lapedriza_EMOTIC_Emotions_in_CVPR_2017_paper.html)

+ PhD Manuscript: [Visual Scene Context in Emotion Perception](https://www.tdx.cat/handle/10803/667808) _[much easier to read with broader set of experiments]_
